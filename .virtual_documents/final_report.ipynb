import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np


import wrangle as w
import explore as ex
import model as m
import evaluate as e

import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

from sklearn.feature_selection import f_regression 
from math import sqrt

#sklearn imports
from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score
from sklearn.metrics import r2_score


# retrieve the data from database or directory 
df = w.get_data()


# clean data, remove nulls, outliers and rename columns
df = w.clean_zillow()


# split the data into train, validate, test
train, validate, test = w.split_data(df)


# overview of data
train.head()


ex.train_heat(train)


null_hypothesis_3 = "There is no linear correlation between the area_sqft and property value."
alternate_hypothesis_3 =  "There is a linear correlation between the area_sqft and property value."


ex.create_relplot(x_var="area_sqft", y_var="property_value", df=train, title="Relationship Between Area and Property Value")


def create_barplot(df, x_var, y_var, title):
    plt.figure(figsize=(8, 6))
    sns.barplot(x=x_var, y=y_var, data=df)
    plt.xlabel(x_var)
    plt.ylabel(y_var)
    plt.title(title)
    plt.show()


null_hypothesis_1 = "The average property value for 1 bathroom homes is less than or equal to the average property value for 2 bedroom homes"
alternate_hypothesis_1 =  "The average property value for 1 bathroom homes is greater than the average property value for 2 bedroom homes"


alpha = 0.05
avg_one_bath = train[train.bathrooms == 1].property_value
avg_two_bed = train[train.bedroom == 2].property_value


ex.create_relpot(train, "bathrooms", "property_value")



graph_title="example"
feature_Q1 = "bathrooms"
target = "property_value"

feature_Q2 = "bedroom"


ex.ind_test(avg_one_bath, avg_two_bed, null_hypothesis_1, alternate_hypothesis_1)


null_hypothesis_2 = "The average property value for homes that have half bathrooms is less than or equal to the average property value for homes with full bathrooms"
alternate_hypothesis_2 =  "The average property value for homes that have half bathrooms is greater than the average property value for homes with full bathrooms"


avg_full_bath  = train[train.full_bath == 1].property_value
avg_half_bath = train[train.full_bath == 0].property_value


create_barplot(train, "full_bath", "property_value")


ex.ind_test(avg_half_bath, avg_full_bath, null_hypothesis_2, alternate_hypothesis_2)


null_hypothesis_4 = "The average property value for homes with pools is less than or equal to the average property value for homes without pools"
alternate_hypothesis_4 =  "The average property value for homes with pools is greater than the average property value for homes without pools"


yes_pool  = train[train.pool == 1].property_value
no_pool = train[train.pool == 0].property_value


create_barplot(train, "pool", "property_value")


ex.ind_test(yes_pool, no_pool, null_hypothesis_4, alternate_hypothesis_4)


null_hypothesis_3 = "The average property value for homes in Orange county is less than or equal to the average property value in other counties"
alternate_hypothesis_3 =  "The average property value for homes in Orange county is greater than the average property value in other counties"


orange_sample = train[train.fips == 'Orange'].property_value
overall_mean = train.property_value.mean()


create_barplot(train, "fips", "property_value")


ex.one_test(orange_sample, overall_mean, null_hypothesis_3, alternate_hypothesis_3)


# features and the target variable
features = ['area_sqft', 'bathrooms', 'bedroom', 'year', 'pool', 'full_bath','orange', 'ventura']
target = "property_value"


# split the train, validate, test into x and y 
x_train, x_validate, x_test  = train[features], validate[features], test[features]
y_train, y_validate, y_test = train[target], validate[target], test[target]


# scaling the x datasets
x_train_scaled, x_validate_scaled, x_test_scaled = w.mm_scale(x_train, x_validate, x_test)


# creating and adding the baseline
baseline = y_train.mean()
baseline_array = np.repeat(baseline, len(train))
rmse, r2 = m.metrics_reg(y_train, baseline_array)

# metrics dataframe
metrics_df = pd.DataFrame(data=[
    {
        'model':'baseline',
        'rmse':rmse,
        'r2':r2
    }
    
])
metrics_df


x_train_scaled


# make the model, fit and use it on train and validate
ols_rmse, ols_r2 = m.ols_mod(x_train_scaled, x_validate_scaled, y_train, y_validate)
# add to metrics_reg
metrics_df.loc[1] = ['ols', ols_rmse, ols_r2]


lars_rmse, lars_r2 = m.lars_mod(x_train_scaled, x_validate_scaled, y_train, y_validate)
metrics_df.loc[2] = ['lars', lars_rmse, lars_r2]


glm_rmse, glm_r2 = m.glm_mod(x_train_scaled, x_validate_scaled, y_train, y_validate)
metrics_df.loc[3] = ['glm', glm_rmse, glm_r2]


ploy_rmse, ploy_r2 = m.poly_mod(x_train_scaled, x_validate_scaled, x_test_scaled, y_train, y_validate)
metrics_df.loc[4] = ['poly', ploy_rmse, ploy_r2]


metrics_df


test_rmse, test_r2 = m.best_model(x_train_scaled, x_test_scaled, y_train, y_test)
metrics_df.loc[5] = ['final test', test_rmse, test_r2]


metrics_df
